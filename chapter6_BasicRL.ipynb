{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 4\n",
      "-1 6\n",
      "-1 9\n",
      "-1 12\n",
      "-1 10\n",
      "-1 11\n",
      "-1 23\n",
      "-1 26\n",
      "-1 28\n",
      "-1 29\n",
      "-1 32\n",
      "-1 34\n",
      "-1 75\n",
      "-1 89\n",
      "-1 92\n",
      "-1 93\n",
      "-1 96\n",
      "-1 97\n",
      "-1 98\n",
      "-1 99\n",
      "100 100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete\n",
    "\n",
    "\n",
    "class SnakeEnv(gym.Env):\n",
    "    SIZE=100\n",
    "  \n",
    "    def __init__(self, ladder_num, dices):\n",
    "        self.ladder_num = ladder_num\n",
    "        self.dices = dices\n",
    "        self.ladders = dict()\n",
    "        self.observation_space=Discrete(self.SIZE+1)\n",
    "        self.action_space=Discrete(len(dices))\n",
    "\n",
    "        ladders = dict(np.random.randint(1, self.SIZE, size=(self.ladder_num, 2)))\n",
    "        \n",
    "        for k,v in ladders.items():\n",
    "            self.ladders[v] = k\n",
    "            self.ladders[k] = v\n",
    "            # print 'ladders info:'\n",
    "            # print self.ladders\n",
    "            # print 'dice ranges:'\n",
    "            # print self.dices\n",
    "        self.pos = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.pos = 1\n",
    "        return self.pos\n",
    "\n",
    "    def step(self, a):\n",
    "        step = np.random.randint(1, self.dices[a] + 1)\n",
    "        self.pos += step\n",
    "        if self.pos == 100:\n",
    "            return 100, 100, 1, {}\n",
    "        elif self.pos > 100:\n",
    "            self.pos = 200 - self.pos\n",
    "\n",
    "        if self.pos in self.ladders:\n",
    "            self.pos = self.ladders[self.pos]\n",
    "        return self.pos, -1, 0, {}\n",
    "\n",
    "    def reward(self, s):\n",
    "        if s == 100:\n",
    "            return 100\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TableAgent(object):\n",
    "    def __init__(self, env):\n",
    "        self.s_len = env.observation_space.n\n",
    "        self.a_len = env.action_space.n\n",
    "\n",
    "        self.r = [env.reward(s) for s in range(0, self.s_len)]\n",
    "        self.pi = np.array([0 for s in range(0, self.s_len)])\n",
    "        self.p = np.zeros([self.a_len, self.s_len, self.s_len], dtype=np.float) # p(s'|s,a)\n",
    "\n",
    "        ladder_move = np.vectorize(lambda x: env.ladders[x] if x in env.ladders else x)\n",
    "\n",
    "        for i, dice in enumerate(env.dices):\n",
    "            prob = 1.0 / dice\n",
    "            for src in range(1, 100):\n",
    "                step = np.arange(dice)\n",
    "                step += src\n",
    "                step = np.piecewise(step, [step > 100, step <= 100],\n",
    "                    [lambda x: 200 - x, lambda x: x])\n",
    "                step = ladder_move(step)\n",
    "                for dst in step:\n",
    "                    self.p[i, src, dst] += prob\n",
    "        self.p[:, 100, 100]=1\n",
    "        self.value_pi = np.zeros((self.s_len))\n",
    "        self.value_q = np.zeros((self.s_len, self.a_len))\n",
    "        self.gamma = 0.8\n",
    "\n",
    "    def play(self, state):\n",
    "        return self.pi[state]\n",
    "\n",
    "\n",
    "class ModelFreeAgent(object):\n",
    "    def __init__(self, env):\n",
    "        self.s_len = env.observation_space.n\n",
    "        self.a_len = env.action_space.n\n",
    "\n",
    "        self.pi = np.array([0 for s in range(0, self.s_len)])\n",
    "        self.value_q = np.zeros((self.s_len, self.a_len))\n",
    "        self.value_n = np.zeros((self.s_len, self.a_len))\n",
    "        self.gamma = 0.8\n",
    "\n",
    "    def play(self, state, epsilon = 0):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(self.a_len)\n",
    "        else:\n",
    "            return self.pi[state]\n",
    "\n",
    "\n",
    "def eval_game(env, policy):\n",
    "    state = env.reset()\n",
    "    return_val = 0\n",
    "    while True:\n",
    "        if isinstance(policy, TableAgent) or isinstance(policy, ModelFreeAgent):\n",
    "            act = policy.play(state)\n",
    "        elif isinstance(policy, list):\n",
    "            act = policy[state]\n",
    "        else:\n",
    "            raise Error('Illegal policy')\n",
    "        state, reward, terminate, _ = env.step(act)\n",
    "        # print state\n",
    "        return_val += reward\n",
    "        if terminate:\n",
    "          break\n",
    "    return return_val\n",
    "\n",
    "\n",
    "env = SnakeEnv(10, [3,6])\n",
    "env.reset()\n",
    "while True:\n",
    "    state, reward, terminate, _ = env.step(0)\n",
    "    print(reward, state)\n",
    "    if terminate == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2 rounds converge\n",
      "return_pi = 71\n",
      "[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "policy_ref = [1] * 97 + [0] * 3\n",
    "policy_0 = [0] * 100\n",
    "policy_1 = [1] * 100\n",
    "\n",
    "class PolicyIteration(object):\n",
    "    \n",
    "    def policy_evaluation(self, agent, max_iter=-1):\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            iteration += 1\n",
    "            new_value_pi = agent.value_pi.copy()\n",
    "            for i in range(1, agent.s_len):\n",
    "                value_sas = []\n",
    "                ac = agent.pi[i]\n",
    "                transition = agent.p[ac, i, :]\n",
    "                value_sa = np.dot(transition, agent.r + agent.gamma * agent.value_pi)\n",
    "                new_value_pi[i] = value_sa\n",
    "            diff = np.sqrt(np.sum(np.power(agent.value_pi - new_value_pi, 2)))\n",
    "            if diff < 1e-6:\n",
    "                break\n",
    "            else:\n",
    "                agent.value_pi = new_value_pi\n",
    "            if iteration == max_iter:\n",
    "                break\n",
    "\n",
    "    def policy_improvement(self, agent):\n",
    "        new_policy = np.zeros_like(agent.pi)\n",
    "        for i in range(0, agent.s_len):\n",
    "            for j in range(0, agent.a_len):\n",
    "                agent.value_q[i, j] = np.dot(agent.p[j, i, :], agent.r + agent.gamma * agent.value_pi)\n",
    "            max_act = np.argmax(agent.value_q[i, :])\n",
    "            new_policy[i] = max_act\n",
    "        if np.all(np.equal(new_policy, agent.pi)):\n",
    "            return False\n",
    "        else:\n",
    "            agent.pi = new_policy\n",
    "            return True\n",
    "\n",
    "    def policy_iteration(self, agent):\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            iteration += 1\n",
    "            self.policy_evaluation(agent)\n",
    "            ret = self.policy_improvement(agent)\n",
    "            if not ret:\n",
    "                break\n",
    "        print('Iter {} rounds converge'.format(iteration))\n",
    "\n",
    "\n",
    "env = SnakeEnv(0, [3, 6])\n",
    "agent = TableAgent(env)\n",
    "pi_algo = PolicyIteration()\n",
    "pi_algo.policy_iteration(agent)\n",
    "\n",
    "print('return_pi = {}'.format(eval_game(env, agent)))\n",
    "print(agent.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer PolicyEval COST: 0.09607505798339844\n",
      "Timer PolicyImprove COST: 0.0018949508666992188\n",
      "return_pi = 10\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    end = time.time()\n",
    "    print('{} COST: {}'.format(name, end - start))\n",
    "\n",
    "\n",
    "class PolicyIterationWithTimer(PolicyIteration):\n",
    "    def policy_iteration(self, agent, max_iter = -1):\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            iteration += 1\n",
    "            with timer('Timer PolicyEval'):\n",
    "                self.policy_evaluation(agent, max_iter)\n",
    "            with timer('Timer PolicyImprove'):\n",
    "                ret = self.policy_evaluation(agent, max_iter)\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "\n",
    "env = SnakeEnv(10, [3,6])\n",
    "agent = TableAgent(env)\n",
    "pi_algo = PolicyIterationWithTimer()\n",
    "pi_algo.policy_iteration(agent)\n",
    "print('return_pi = {}'.format(eval_game(env, agent)))\n",
    "print(agent.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 94 rounds converge\n",
      "return_pi=92\n",
      "[0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "class ValueIteration(object):\n",
    "    def value_iteration(self, agent, max_iter = -1):\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            iteration += 1\n",
    "            new_value_pi = np.zeros_like(agent.value_pi)\n",
    "            for i in range(1, agent.s_len): # for each state\n",
    "                value_sas = []\n",
    "                for j in range(0, agent.a_len): # for each act\n",
    "                    value_sa = np.dot(agent.p[j, i, :], agent.r + agent.gamma * agent.value_pi)\n",
    "                    value_sas.append(value_sa)\n",
    "                new_value_pi[i] = max(value_sas)\n",
    "            diff = np.sqrt(np.sum(np.power(agent.value_pi - new_value_pi, 2)))\n",
    "\n",
    "            if diff < 1e-6:\n",
    "                break\n",
    "            else:\n",
    "                agent.value_pi = new_value_pi\n",
    "            if iteration == max_iter:\n",
    "                break\n",
    "        print('Iter {} rounds converge'.format(iteration))\n",
    "        for i in range(1, agent.s_len):\n",
    "            for j in range(0, agent.a_len):\n",
    "                agent.value_q[i, j] = np.dot(agent.p[j, i, :], agent.r + agent.gamma * agent.value_pi)\n",
    "            max_act = np.argmax(agent.value_q[i, :])\n",
    "            agent.pi[i] = max_act\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "env = SnakeEnv(10, [3,6])\n",
    "agent = TableAgent(env)\n",
    "vi_algo = ValueIteration()\n",
    "vi_algo.value_iteration(agent)\n",
    "print('return_pi={}'.format(eval_game(env,agent)))\n",
    "print(agent.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer PolicyEval COST: 0.09385323524475098\n",
      "Timer PolicyImprove COST: 0.0014247894287109375\n",
      "Timer PolicyIter COST: 0.0954427719116211\n",
      "return_pi=88\n",
      "===========================\n",
      "Iter 94 rounds converge\n",
      "Timer ValueIter COST: 0.1848917007446289\n",
      "return_pi=92\n",
      "===========================\n",
      "Iter 10 rounds converge\n",
      "Timer PolicyEval COST: 0.0010371208190917969\n",
      "Timer PolicyImprove COST: 0.0012159347534179688\n",
      "Timer GeneralizedIter COST: 0.027844667434692383\n",
      "return_pi=92\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# general iteration\n",
    "class GeneralizedPolicyIteration(object):\n",
    "    def __init__(self):\n",
    "        self.pi_algo = PolicyIterationWithTimer()\n",
    "        self.vi_algo = ValueIteration()\n",
    "\n",
    "    def generalized_policy_iteration(self, agent):\n",
    "       self.vi_algo.value_iteration(agent, 10)\n",
    "       self.pi_algo.policy_iteration(agent, 1)\n",
    "\n",
    "def policy_iteration_demo():\n",
    "    np.random.seed(0)\n",
    "    env = SnakeEnv(10, [3,6])\n",
    "    agent = TableAgent(env)\n",
    "    pi_algo = PolicyIterationWithTimer()\n",
    "    with timer('Timer PolicyIter'):\n",
    "        pi_algo.policy_iteration(agent)\n",
    "    print('return_pi={}'.format(eval_game(env,agent)))\n",
    "\n",
    "def value_iteration_demo():\n",
    "    np.random.seed(0)\n",
    "    env = SnakeEnv(10, [3,6])\n",
    "    agent = TableAgent(env)\n",
    "    pi_algo = ValueIteration()\n",
    "    with timer('Timer ValueIter'):\n",
    "        pi_algo.value_iteration(agent)\n",
    "    print('return_pi={}'.format(eval_game(env,agent)))\n",
    "\n",
    "def generalized_iteration_demo():\n",
    "    np.random.seed(0)\n",
    "    env = SnakeEnv(10, [3,6])\n",
    "    agent = TableAgent(env)\n",
    "    pi_algo = GeneralizedPolicyIteration()\n",
    "    with timer('Timer GeneralizedIter'):\n",
    "        pi_algo.generalized_policy_iteration(agent)\n",
    "    print('return_pi={}'.format(eval_game(env,agent)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    policy_iteration_demo()\n",
    "    print('===========================')\n",
    "    value_iteration_demo()\n",
    "    print('===========================')\n",
    "    generalized_iteration_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
